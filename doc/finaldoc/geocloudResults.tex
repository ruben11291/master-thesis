\section{GEO-Cloud Experiment Results}
\label{sec:geocloud-results}

The scope of this project is to develop and implement in a Federated Future
Internet infrastructure a simulation environment that allows experimenters
testing different realistic Earth Observation scenarios, in which a
constellation of satellites record images of the world surface in a daily basis
they are downloaded to a distributed network of ground stations to be processed and
distributed through a data center computed in cloud.

The experiment carried out to demonstrate the correct implementation of the
GEO-Cloud system consisted in the execution of the ``Scenario 1: Emergencies-Lorca Earthquake'' (see Section~\ref{anex:scenarios}). This scenario focuses in the Earthquake that
took place in the city of Lorca in May 11th 2011. When the natural disaster occurred
it was required to have as soon as possible recent images of the affected area in
order to assess the damages and plan the emergency services
involvement. Satellite imagery has been proven to be an indispensable tool to
face natural disasters. Satellite imagery is then required before, during and
after the disaster occurred. 

Firstly, the whole system was started and configured, and the graphical user
interface executed. Notice that the Ground Stations of the \sss was configured with
the network impairments measured in the PlanetLab experiment (see
Section~\ref{sec:pl-res}). The Table~\ref{table:gs-results} shows these values.

\begin{table}[!h]
  \centering
  {\small
  \input{tables/gs-results.tex}
  }
  \caption{Impairments implemented in \emph{Virtual Wall} in the networks between the ground stations and the BonFIRE cloud.}
  \label{table:gs-results}
\end{table}



Once the experiment was started by clicking the start button in the \ac{GUI} the system started working in an integrated fashion:
\begin{itemize}
\item The satellite connects with the ground station.
\item The satellite downloads the acquired data to the ground station.
\item The cloud ingests, process, archive and catalogue the images resulting
  from processing the acquired data by the satellites.
\end{itemize}


In the simulation of the scenario, the \vw nodes were reached and they received
the command to start the scenario. These nodes are the 12 ground stations and
the constellation formed by 17 satellites. The ground stations were initialized by
the \emph{GUI} by sending the identity of each ground station, and
the scenario. 
The same occurs for the satellite constellation. The \ac{GUI} sends to each
satellite its id and the scenario to be simulated.
 The \satss and the \gsss started their executions. The satellites acted regarding the data obtained from the \ac{STK} software. 
There are different behaviours depending on the zone in which the satellite enters (see
Section~\ref{sec:spaceSystemSimulator}). In the Scenario 1, the satellites which
enters into any Ground Station visibility zone are: 1, 7, 4, 5 twice and 11
twice. The
durations of each access are depicted in the Table~\ref{table:accesses-scenario1}.
\begin{table}[!h]
  \centering
  {\small
  \input{tables/accesses-scenario1.tex}
  }
  \caption{Satellite accesses in Scenario 1}
  \label{table:accesses-scenario1}
\end{table}

The only satellite that acquires \ac{AOI} in this scenario is the satellite
11. The others satellites enters in their corresponding footprints and they
download the acquire non \ac{AOI} raw data files into the Ground Stations. When the satellite 11 enters into the Puertollano footprint, a connection with
the Puertollano Ground Station is stablished. The satellite
11 starts imaging at $1395~Mbps$ the Lorca surface when it overflies Spain and at the same time, it
starts to download at $160~Mbps$ the the recorded data to the Puertollano ground station. During the
duration of this scenario (23.4 seconds) an image of the Lorca region is
acquired and downloaded in the Puertollano ground station. The other satellites
download at $160~Mbps$ non \ac{AOI} data until the scenario ends. 

When the satellite 11 leaves of the Puertollano footprint, the ground station
counts the received packets. These packets are ``AOI'' packets, so all the data
belong to \ac{AOI} area. As a result, an \ac{AOI} raw data file is created in the
Puertollano ground station. 
The other satellites download non ``AOI'' packets during the scenario duration
at $160~Mbps$. For example, the Svalbard ground stations receives the packets
from the satellite 12. This ground station creates a non \ac{AOI} image in its
memory.

The features of raw data are summarized as follows:
\begin{itemize}
\item The raw data occupied $302~MB$. 
\item The raw data represented a scene acquired by a satellite. A scene is a
  piece of the Earth surface. In this case represented the Lorca \ac{AOI} zone.
\item The raw data scene is splitted into 8 sectors. A scene is a set of metadata and
  values representing a piece of the total image acquired.
\item The raw data is conformed by the data of the different spectral bands
  together the metadata. The metadata are the geolocation data and the
  decompression data among others.
\end{itemize}


Meanwhile, the \emph{Orchestrator} is periodically polling the ground stations
\ac{FTP} connections for detecting when an image is completely downloaded in any
of them. When it detected that the Puertollano ground station created an \emph{AOI}
image, it automatically downloaded it. For the others ground stations occurred
the same. Notice that available bandwidth between a \vw node (ground station) and a \bonfire
node (the \emph{Orchestrator})  was around $235~Mb/s$, however the simulated
bandwidth was customized with the results showed in Table~\ref{table:gs-results}. 

Then, when the \ac{AOI} image was available in the Puertollano ground station,
it was downloaded through the \emph{FTP} connection by the \emph{Orchestrator}. The \emph{Orchestrator}
queued the raw data for processing when the \emph{Processing Chain} module was free,
the \emph{Orchestrator} sent that data for processing. When the
\emph{Processing Chain} was busy, the \emph{Orchestrator} queued the raw data until
the \emph{Processing Chain} were free. 
As the Lorca image is the first acquired raw data of the scenario, it was
directly sent to the \emph{Processing Chain} to be processed. The \bonfire machines are interconnected through  \emph{Gigabit Ethernet}
interfaces, so that was  the transfer data rate between the  \emph{Orchestrator}
and the \emph{Processing Chain} was high. Note that in the implementation using a shared storage,
this data sending was avoided because the \emph{Orchestrator} only sends the path
of the raw data to the \emph{Processing Chain}, so this sending time was nil.

Once the raw data was sent to the \emph{Processing Chain}, the transformation from
raw data to a geolocated and orthorectified image started. In this implementation the
on demand creation of the \emph{Processing Chains} could not be implemented the
\emph{Fed4FIRE}  tools devoted to that purpose were not operative yet.
Thus, when the raw data was in  the \emph{Processing Chain} component, it automatically started the processing. The processing accomplishes the L0, L0R,
L1A, L1BG, L1BR, L1C stages. 
The functions of the different stages are explained
in Section~\ref{subsub:processors}. As summary, the functions of the
\emph{Product Processors} were the following:
\begin{itemize}
\item \emph{Level 0} obtained unprocessed images, in digital count numbers.
\item \emph{Level L1A} obtained calibrated products, in units of radiance.
\item \emph{Level L1B} obtained calibrated and geometrically corrected products
  (ortho-rectified), blindly geolocated. There are two L1B processors: L1BG and
  L1BR. Both \emph{Product Processors} do the same but the first in manual
  configuration and the second one, automatically.
\item \emph{Level L1C} obtained calibrated and geometrically corrected products (ortho-rectified), precisely geolocated using ground control points.
\end{itemize}


The required times obtained for
each \emph{Product Processor} are shown in the Table~\ref{table:p-times}. In
this case, these \emph{Product Processors} were implemented with a datablock
storage as background file system. These
times were obtained by sampling and these are the mean values. The first column of the table ``User Time'' shows the  time of the
\emph{Product Processors} used to the image process. The second column ``System
Time'' represents the time that the system required to perform the system calls. The ``Elapsed Time'' is
the total time required to process the image at each different level. The times of every I/O operations are
included in it. The ``RAM'' column represents the required \emph{RAM} memory to
perform the process. The last column ``CPU'' is the spent \ac{CPU} for
processing this stage.

\begin{table}[!h]
  \centering
  {\small
  \input{tables/processor-times.tex}
  }
  \caption{Processing times of each product processor in the datablock storage}
  \label{table:p-times}
\end{table}

As it can be observed in the table, the \emph{RAM} required for each
\emph{Product Processor} is different. This is because the actions performed by
the \emph{Product Processors} are different. For example on the other hand, the L0
stage slightly process the raw data and the other hand, the L1BR processor
accomplishes the geolocation of the image, so the \emph{CPU} activity is more intense. 
The same is applied for the use of the \emph{RAM} memory, which depends on the
\emph{Product Processor} actions.
Furthermore, the
uses of the \ac{CPU} sometimes overloaded 100\%. This is occurred because the processing
required more than one core and this was automatically provided by the cloud
system. Thus a workload  over 100\% means that the
\emph{Product Processor} occuped more than a node to carry out the task.

The times required by each \emph{Product Processor} were also different. The L1B
required more time than the others because it accomplished the image
geometric corrections. The actions performed by each \emph{Product Processor} are
explained in Section~\ref{subsub:processors}. 



Consecuently, these results are contrasted with the times obtained when executing
the processors in the shared storage. These results are depicted in Table~\ref{table:p-times-shared}
and there were found differences in terms of processing time.

\begin{table}[!h]
  \centering
  {\small
  \input{tables/processor-times-shared.tex}
  }
  \caption{Processing times of each product processor in the shared storage}
  \label{table:p-times-shared}
\end{table}

The time  required by each \emph{Product Processor} required when using a shared
storage as file system was higher. The difference consisted of the \ac{NFS} in which
the shared storage was implemented, was located
in other \emph{Fed4FIRE} testbed. This testbed was the \emph{IBBT} platform which is located in
Ghent (Belgium), so all the \emph{I/O} operations such as reads and writes were performed
using the Internet network. The delays on the communications, the congestion of
the network and the packets distribution algorithms of the network provoked more
delays than when performing locally or when using the shared storage.  

To corroborate that hypotesis, a performance test in the shared storage was carried out. This test
consisted of executing the following command:
\begin{itemize}
\item[>] dd if=/dev/zero of=/mnt/shared/test-performance bs=1M count=2560
  conv=fdatasync
\end{itemize}
It measured the write rate to the shared storage. The result obtained was the
following:
\begin{itemize}
\item 2684354560 bytes (2.7 GB) copied, 1174.6 s, 2.3 MB/s
\end{itemize}
2.7 GB of data was copied in $1174.6~s$, obtaining a data rate  of $2.3~MB/s$ between the
\bonfire machine and the store located in \emph{IBBT}. 

Then, the command was repeated for executing in the local storage as follows:
\begin{itemize}
\item[>] dd if=/dev/zero of=/mnt/local/test-performance bs=1M count=2560
  conv=fdatasync
\end{itemize}

It measured the write rate to the shared storage. The result obtained was the
following:
\begin{itemize}
\item 2684354560 bytes (2.7 GB) copied, 50.975 s, 52.7 MB/s
\end{itemize}
2.7 GB of data was copied in $50.97~s$, obtaining a data rate  of $2.3~MB/s$ between the \bonfire machine and
the store located in  \bonfire or in the machine.

Thus it is concluded that the bottleneck obtained in the products processors
implemented in the shared storage was due to the implementation of the \ac{NFS}
storage which the
\emph{Fed4FIRE} testbed implemented.

As a result, the image of \emph{Lorca} was processed in the following
times:
\begin{itemize}
\item Using a datablock storage: $2~h:17':43''$
\item Using a shared storage: $17~h:56':43''$
\end{itemize}


Once the \ac{AOI} Lorca image was obtained, the \emph{Processing Chain} sent it
to the \emph{Archive and Catalogue} module. The transfer data rate between
\bonfire machines was $1~Gbps$, so the transferring time was short. By using the shared storage for
implementing the background filesystem, this transfer time was avoided. It was
stimated that this transferring time was short. The \emph{Archive
  and Catalogue} stored and catalogued the image. A workspace and a data store
were created for storing the image. The \emph{Archive and Catalogue} received
the image and automatically created the workspace and the data store where the
image was located. Once the image was in \emph{GeoServer},
layers, tiles and image pyramids were automatically created by the software. The cataloguing time of any image
was near to zero in the local implementation of the storage, and it was nil in the implementation of the shared
storage because the image was already in the filesystem. 
The next step consisted of publishing the catalogue by using
a \ac{CSW} service and the web interface. By using the web-browser, the Lorca
image was visualized in the map-view of \emph{GeoServer}. The image was
catalogued in a workspace and it was accesible through the Internet in the
following address:
\begin{itemize}
\item \url{http://IP\_address\_GeoServer\/geoserver}
\end{itemize}
where the \emph{IP\_address\_GeoServer} was the \ac{IP} address of the
\emph{Archive and Catalogue} machine.

Furthermore, the implemented architecture in ICE was checked. A client
to test that architecture was accomplished. The client was executed and all the
components worked as expected. The  \emph{Orchestrator}
component ingested the images acquired by the satellites in the system, the
\emph{Processing Chains} transformed the raw data to orthorectified and
geolocated images, and finally the \emph{Archive and Catalogue} performed the
archiving and cataloguing successfully. 

Finally, a video of the real time system execution named
``video-demo'' is included in the ``video''
folder in the attached CD-ROM.

